{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 9.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.22.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.22.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.22.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pickle-mixin\n",
      "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pickle-mixin\n",
      "  Building wheel for pickle-mixin (setup.py): started\n",
      "  Building wheel for pickle-mixin (setup.py): finished with status 'done'\n",
      "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6008 sha256=d61d53a895e770ff03b2309e88e4a6b9aa4fc2aad54b4d1546c7b5fad14591c3\n",
      "  Stored in directory: c:\\users\\hashi\\appdata\\local\\pip\\cache\\wheels\\3e\\c6\\e9\\d1b0a34e1efc6c3ec9c086623972c6de6317faddb2af0a619c\n",
      "Successfully built pickle-mixin\n",
      "Installing collected packages: pickle-mixin\n",
      "Successfully installed pickle-mixin-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies \n",
    "%pip install --upgrade pip\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install pickle-mixin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pk\n",
    "import os.path as path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "      <th>3072</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.615686</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.427451</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.427451</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.925490</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.909804</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.207843</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.137255  0.105882  0.098039  0.101961  0.094118  0.086275  0.090196   \n",
       "1     0.078431  0.078431  0.070588  0.074510  0.082353  0.090196  0.090196   \n",
       "2     0.454902  0.450980  0.607843  0.556863  0.388235  0.454902  0.552941   \n",
       "3     0.384314  0.356863  0.321569  0.270588  0.231373  0.235294  0.258824   \n",
       "4     0.333333  0.400000  0.427451  0.462745  0.192157  0.054902  0.129412   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  0.925490  0.913725  0.913725  0.913725  0.913725  0.913725  0.913725   \n",
       "9996  0.905882  0.905882  0.909804  0.901961  0.901961  0.905882  0.917647   \n",
       "9997  0.498039  0.545098  0.607843  0.584314  0.517647  0.619608  0.639216   \n",
       "9998  0.745098  0.784314  0.815686  0.815686  0.815686  0.815686  0.811765   \n",
       "9999  0.694118  0.682353  0.713725  0.737255  0.729412  0.741176  0.772549   \n",
       "\n",
       "          7         8         9     ...      3063      3064      3065  \\\n",
       "0     0.098039  0.098039  0.094118  ...  0.686275  0.682353  0.682353   \n",
       "1     0.090196  0.090196  0.090196  ...  0.407843  0.466667  0.615686   \n",
       "2     0.435294  0.431373  0.478431  ...  0.070588  0.054902  0.031373   \n",
       "3     0.400000  0.392157  0.364706  ...  0.180392  0.341176  0.427451   \n",
       "4     0.168627  0.117647  0.101961  ...  0.482353  0.411765  0.345098   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.913725  0.913725  0.913725  ...  0.741176  0.745098  0.752941   \n",
       "9996  0.925490  0.921569  0.921569  ...  0.525490  0.564706  0.576471   \n",
       "9997  0.619608  0.623529  0.556863  ...  0.788235  0.772549  0.772549   \n",
       "9998  0.800000  0.796078  0.796078  ...  0.639216  0.631373  0.635294   \n",
       "9999  0.803922  0.831373  0.843137  ...  0.227451  0.133333  0.207843   \n",
       "\n",
       "          3066      3067      3068      3069      3070      3071  3072  \n",
       "0     0.670588  0.670588  0.662745  0.662745  0.658824  0.658824   1.0  \n",
       "1     0.639216  0.274510  0.258824  0.435294  0.380392  0.200000   6.0  \n",
       "2     0.023529  0.019608  0.019608  0.070588  0.329412  0.486275   6.0  \n",
       "3     0.443137  0.439216  0.470588  0.490196  0.517647  0.541176   8.0  \n",
       "4     0.345098  0.333333  0.278431  0.250980  0.254902  0.258824   8.0  \n",
       "...        ...       ...       ...       ...       ...       ...   ...  \n",
       "9995  0.752941  0.768627  0.768627  0.772549  0.776471  0.764706   8.0  \n",
       "9996  0.576471  0.631373  0.674510  0.666667  0.666667  0.690196   8.0  \n",
       "9997  0.784314  0.780392  0.780392  0.772549  0.752941  0.749020   7.0  \n",
       "9998  0.635294  0.635294  0.635294  0.639216  0.713725  0.752941   2.0  \n",
       "9999  0.333333  0.415686  0.411765  0.466667  0.498039  0.533333   5.0  \n",
       "\n",
       "[10000 rows x 3073 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading and formatting cifar-10 dataset\n",
    "\n",
    "# utility function to read a batch file in the cifar-10-batches-py directory\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pk.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# utility function to gaussian norm numpy array to range [0, 1]\n",
    "def norm(data: np.ndarray):\n",
    "    norm = (data - np.mean(data)) / np.std(data)\n",
    "    return (norm - np.min(norm)) / (np.max(norm) - np.min(norm))\n",
    "\n",
    "\n",
    "# read and normalize train and test data\n",
    "def read_train_test(dir: str='./cifar-10-batches-py', nrm: bool=True):\n",
    "    TRAIN = []\n",
    "    for i in range(5):\n",
    "        batch = unpickle(path.join(dir, f'data_batch_{i+1}'))\n",
    "        data, labels = batch[b'data'], np.reshape(np.array(batch[b'labels']), (10000, 1))\n",
    "        if nrm: data = norm(data)\n",
    "        TRAIN.append(np.hstack((data, labels)))\n",
    "\n",
    "    # train split with shape 5, 100000, 3073 \n",
    "    # 5 batches\n",
    "    # 100000 images\n",
    "    # 1024 red, 1024 blue, 1024 green, 1 label\n",
    "    TRAIN = np.array(TRAIN)\n",
    "\n",
    "    # reading test data same as abv\n",
    "    test_batch = unpickle(path.join(dir, 'test_batch'))\n",
    "    test_data, test_labels = test_batch[b'data'], np.reshape(np.array(test_batch[b'labels']), (10000, 1))\n",
    "    if nrm: test_data = norm(test_data)\n",
    "    TEST = np.hstack((test_data, test_labels))\n",
    "\n",
    "    return TRAIN, TEST\n",
    "\n",
    "TRAIN, TEST = read_train_test()\n",
    "\n",
    "# example, batch 1 from train split\n",
    "batch1_df = pd.DataFrame(TRAIN[1, :, :])\n",
    "batch1_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:            (3072, 1)       activation: null\n",
      "m1_dim:               (64, 3072)      activation: relu           \n",
      "m2_dim:               (64, 65)        activation: relu           \n",
      "m2_dim:               (10, 65)        activation: softmax        \n",
      "output_dim:           (10, 1)         activation: null\n",
      "num_parameters:       201418\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hashi\\AppData\\Local\\Temp\\ipykernel_21016\\1458219953.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w = np.array(w)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# relu activation function\n",
    "# relu = lambda x : x if x > 0 else 0\n",
    "def relu(x):\n",
    "    return x if x > 0 else 0\n",
    "\n",
    "# softmax activation function\n",
    "def softmax():\n",
    "    pass\n",
    "\n",
    "# vanilla neural network / mlp\n",
    "class mlp:\n",
    "\n",
    "    # default network structure\n",
    "    # input data            x:  N x 1\n",
    "    # input -> hidden1      m1: 64 x N   b1: 64 x 1\n",
    "    # hidden1 -> hidden2    m2: 64 x 64  b2: 64 x 1\n",
    "    # hidden2 -> output     m3: 10 x 64  b3: 10 x 1\n",
    "    # y_h = softmax(m3*relu(m2*(relu(m1*x+b1))+b2)+b3)\n",
    "\n",
    "    # notes\n",
    "    # w weight matrix for the mlp\n",
    "    # w is ragged since not each layer has the same dimension feature matrix\n",
    "    # bias b1 b2 b3 are included in m1 m2 m3\n",
    "\n",
    "    def __init__(self, hidden_activation=relu, output_activation=softmax, hidden_layers=2, hidden_dim=64, N=3072, output_dim=10):\n",
    "\n",
    "        # initialize random weights for each layers feature matrix\n",
    "        w = []\n",
    "        m1= np.random.randn(hidden_dim, N)\n",
    "        w.append(m1)\n",
    "\n",
    "        for _ in range(hidden_layers-1):\n",
    "            m = np.random.randn(hidden_dim, hidden_dim+1)\n",
    "            w.append(m)\n",
    "\n",
    "        mn = np.random.randn(output_dim, hidden_dim+1)\n",
    "        w.append(mn)\n",
    "        w = np.array(w)\n",
    "\n",
    "        self.w = w\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.input_dim = N\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # finding number of parameters / weights\n",
    "        self.num_params = sum(fm.shape[0]*fm.shape[1] for fm in w)\n",
    "        \n",
    "\n",
    "    def fit(self) -> None:\n",
    "        pass\n",
    "\n",
    "    # forward propogation\n",
    "    # image data    x: 3072 x 1\n",
    "    # prediction    y_h: 1 x 1\n",
    "    def predict(self, x) -> int:\n",
    "        y_h = x\n",
    "        for i in range(len(self.w) - 1):\n",
    "            y_h = relu(y_h @ self.w[i, :, :-1] + self.w[i, :, -1])\n",
    "        \n",
    "        y_h = softmax(y_h @ self.w[i+1, :, :-1] + self.w[i+1, :, -1])\n",
    "        \n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        res = ''\n",
    "\n",
    "        res += f\"{'input_dim:':<22}{str((self.input_dim, 1)):<15} {'activation: null':<15}\\n\"\n",
    "        # feature matrix 1 summary\n",
    "        # hidden layer summaries\n",
    "        for i in range(len(self.w)-1):\n",
    "            res += f\"{'m'+ str(i+1) + '_dim:':<22}{str(self.w[i].shape):<15} activation: {str(self.hidden_activation.__name__):<15}\\n\"\n",
    "        \n",
    "        # output feature matrix summary\n",
    "        res += f\"{'m' + str(i+1) + '_dim:':<22}{str(self.w[i+1].shape):<15} activation: {str(self.output_activation.__name__):<15}\\n\"\n",
    "        res += f\"{'output_dim:':<22}{str((self.output_dim, 1)):<15} {'activation: null':<15}\\n\"\n",
    "\n",
    "        # num of trainable parameters \n",
    "        res += f\"{'num_parameters:':<22}{self.num_params}\\n\"\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "nn = mlp()\n",
    "print(nn)\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b79a9603654acc1a1dda7ab9d7fbeea5670a7835087b35cd5f08522a1762abf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
